{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92a83860",
   "metadata": {},
   "source": [
    "# CIDS-Sim with Non-IID data\n",
    "\n",
    "Federated Learning involves collaboration among multiple clients to learn from decentralized data. In the context of CIDS, each client acts as a detector unit distributed across various networks, while the central server, responsible for aggregating the models, functions as the correlation unit. This research employs a non-IID (Non-Independent and Identically Distributed) data setting to distribute data across different clients. The Federated Averaging (FedAvg) algorithm is used to aggregate models from multiple clients.\n",
    "\n",
    "## CIDS Architecture\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"699\" alt=\"image\" src=\"/images/arch_CIDS-Sim_Non-IID.png\">\n",
    "</p>\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "This simulator will used Coordinated Attack dataset with [NetFlow (NF) features](https://drive.google.com/file/d/1xioZGRQKYbrpiBhkd56sHxFtdn4rEiTt/view) and [CICFlowMeter (CIC) features](https://www.unb.ca/cic/datasets/ids-2018.html).  For the NF dataset, please download from [here](https://www.kaggle.com/luminardata) and for the CIC dataset, please download from [here](https://www.kaggle.com/luminardata)\n",
    "\n",
    "## Other Information\n",
    "\n",
    "The simulator will run binary classification, so the traffic will labeled as normal (0) or anomaly (1)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b074c",
   "metadata": {},
   "source": [
    "First, import libraries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31094e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.metrics import Recall, Precision\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af891724",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Load Coordinated Attack dataset. Recommended using \".parquet\" file for faster reading data.\n",
    "\n",
    "Reference:\n",
    "1. https://parquet.apache.org/docs/overview/\n",
    "2. https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html\n",
    "3. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html \n",
    "\n",
    "Use `CoAt-Set_NF.parquet` or `CoAt-Set_NF.csv` for dataset with NF feature.\n",
    "\n",
    "Use `CoAt-Set_CIC.parquet` or `CoAt-Set_CIC.csv` for dataset with CIC feature.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e424dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to read dataset using parquet file (default)\n",
    "df = pd.read_parquet('./dataset/CoAt-Set_NF.parquet', engine='pyarrow')\n",
    "\n",
    "# Use this if you want to read dataset using CSV file\n",
    "# df = pd.read_csv('./dataset/CoAt-Set_NF.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b0c07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "View the dataset information\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056fa151",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df1bee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Look for normal (0) and anomaly (1) traffic distribution\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c048bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3267a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "X and y are used to represent the input features and the corresponding target labels, respectively.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = df.drop(columns=['Label'])\n",
    "y_df = df['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3fd022",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Scaling data to ensures that features have values in the same range\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbcbff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this scaler for NF dataset\n",
    "scaler = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "# Use this scaler for CIC dataset\n",
    "#scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5abdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_scl = scaler.fit_transform(X_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c03c079",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This Python function, `load_data(client_id)`, is designed to load a portion of data for a specific client in a Federated Learning setting where non-IID data is distributed among different clients. Here's a step-by-step explanation of how the code works:\n",
    "\n",
    "### Function Purpose:\n",
    "- The function returns a small, random subset of the dataset (`X_df_scl` and `y_df`) based on the `client_id`. The randomness is seeded by the `client_id`, ensuring that the data split is consistent and non-overlapping for each client, simulating non-IID data distribution across clients.\n",
    "\n",
    "### Line-by-Line Explanation:\n",
    "1. **`def load_NFUQ_data(client_id):`**\n",
    "   - The function `load_NFUQ_data` takes a single argument, `client_id`, which is a unique identifier for each client in a distributed learning setting.\n",
    "\n",
    "2. **`np.random.seed(client_id)`**\n",
    "   - This sets the random seed using the `client_id`, ensuring that every client receives a unique, reproducible subset of the data. Using the same `client_id` will result in the same split each time.\n",
    "\n",
    "3. **`indices = np.arange(len(X_df_scl))`**\n",
    "   - This creates an array of indices that represent the positions of data samples in the dataset `X_df_scl`. `np.arange(len(X_df_scl))` generates indices from 0 to the number of rows in `X_df_scl`.\n",
    "\n",
    "4. **`np.random.shuffle(indices)`**\n",
    "   - This shuffles the array of indices randomly, ensuring that data is selected in a non-sequential manner, making the data split for each client random and unique.\n",
    "\n",
    "5. **`fraction = 0.02`**\n",
    "   - This sets a fraction (2%) of the total dataset that will be assigned to the client. In this case, each client receives 2% of the total dataset.\n",
    "\n",
    "6. **`client_data_size = int(fraction * len(X_df_scl))`**\n",
    "   - This calculates the number of data samples the client will receive by multiplying the fraction (2%) by the total number of samples in `X_df_scl`.\n",
    "\n",
    "7. **`client_indices = indices[:client_data_size]`**\n",
    "   - This selects the first `client_data_size` indices from the shuffled `indices` array, which correspond to the data samples that will be assigned to the client.\n",
    "\n",
    "8. **`X_client = X_df_scl[client_indices]`**\n",
    "   - This selects the features (input data) for the client using the previously selected indices from the `X_df_scl` dataset, resulting in `X_client`, the feature data for the client.\n",
    "\n",
    "9. **`y_client = y_df.iloc[client_indices]`**\n",
    "   - This selects the corresponding labels (target data) from `y_df` using the same indices, resulting in `y_client`, the label data for the client.\n",
    "\n",
    "10. **`return X_client, y_client`**\n",
    "    - Finally, the function returns the subset of features (`X_client`) and labels (`y_client`) for the given client.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4d07ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(client_id):\n",
    "    \n",
    "    # Create non-IID splits based on client_id\n",
    "    np.random.seed(client_id)\n",
    "    indices = np.arange(len(X_df_scl))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Choose a fraction of the data for this client\n",
    "    fraction = 0.02\n",
    "    client_data_size = int(fraction * len(X_df_scl))\n",
    "    client_indices = indices[:client_data_size]\n",
    "\n",
    "    X_client = X_df_scl[client_indices]\n",
    "    y_client = y_df.iloc[client_indices]\n",
    "\n",
    "    return X_client, y_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc5d3a",
   "metadata": {},
   "source": [
    "The `create_model()` function defines a simple neural network model using Keras, following the pyramid method for the number of layers, where each successive layer has fewer neurons than the previous one. Hereâ€™s a breakdown:\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Function Definition:**\n",
    "   - `create_model(input_shape)` takes `input_shape` as an argument, representing the shape of the input data. It defines a neural network model with a series of fully connected (dense) layers.\n",
    "\n",
    "2. **Keras Sequential Model:**\n",
    "   - `keras.Sequential()` is a Keras model where layers are stacked linearly, one after the other.\n",
    "\n",
    "3. **Layers (Pyramid Structure):**\n",
    "   - The pyramid method refers to gradually reducing the number of neurons (or units) as the layers progress deeper in the network, resembling a pyramid shape. This can help the model learn hierarchical representations of the data, reducing the dimensionality step by step.\n",
    "   \n",
    "   Here, the layers follow this pattern:\n",
    "   \n",
    "   - **Input Layer:** \n",
    "     - `layers.Dense(20, activation='relu', input_shape=(input_shape,))` â€” The first layer has 20 neurons and expects the input shape specified by `input_shape`. The activation function is ReLU (Rectified Linear Unit), commonly used to introduce non-linearity.\n",
    "   \n",
    "   - **Hidden Layers:**\n",
    "     - `layers.Dense(10, activation='relu')` â€” The second layer has 10 neurons and uses ReLU.\n",
    "     - `layers.Dense(5, activation='relu')` â€” The third layer has 5 neurons and uses ReLU.\n",
    "     - `layers.Dense(3, activation='relu')` â€” The fourth layer has 3 neurons and uses ReLU.\n",
    "\n",
    "   - **Output Layer:**\n",
    "     - `layers.Dense(1, activation='sigmoid')` â€” The final output layer has 1 neuron and uses the sigmoid activation function, which is typically used for binary classification (as the output will be between 0 and 1).\n",
    "\n",
    "4. **Model Compilation:**\n",
    "   - The model is compiled with the following parameters:\n",
    "     - **Loss Function:** `mean_squared_error` â€” Measures the mean of the squares of the errors between the predicted and actual values, though typically `binary_crossentropy` would be more common in classification tasks.\n",
    "     - **Optimizer:** `sgd` (Stochastic Gradient Descent) â€” Used for updating the weights during training.\n",
    "     - **Metrics:** `accuracy`, `Recall()`, and `Precision()` â€” These are the metrics used to evaluate the model's performance during training. Accuracy measures how many predictions are correct, while recall and precision are related to the model's performance on imbalanced datasets.\n",
    "\n",
    "### Pyramid Layer Structure:\n",
    "- The number of neurons decreases from 20 to 1 across the layers, following a decreasing trend like a pyramid. This structure often helps the model reduce dimensionality and focus on the most important features as it processes data through the layers.\n",
    "\n",
    "### Overall:\n",
    "This is a basic feed-forward neural network with a pyramid structure, which is useful for gradually condensing the feature space, moving from broader representations to more specific ones, ultimately leading to a single output. The model is compiled for binary classification with additional performance metrics (recall and precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ae365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network model\n",
    "def create_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(20, activation='relu', input_shape=(input_shape,)),\n",
    "        layers.Dense(10, activation='relu'),\n",
    "        layers.Dense(5, activation='relu'),\n",
    "        layers.Dense(3, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy', Recall(), Precision()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d3023",
   "metadata": {},
   "source": [
    "The `cids_federated_training()` function implements the **training process for a Collaborative Intrusion Detection System (CIDS) using Federated Learning**. The goal is to train a global model based on the local training of models across multiple distributed nodes, without sharing raw data. Hereâ€™s a step-by-step explanation of how it works:\n",
    "\n",
    "### 1. **Function Definition:**\n",
    "   - **`cids_federated_training(num_nodes=5, num_rounds=5)`**:\n",
    "     - This function performs federated learning for intrusion detection across `num_nodes` (clients or devices) over `num_rounds` (iterations of federated learning).\n",
    "     - Each node trains a local model, and the weights from all nodes are averaged to update a global model in each round.\n",
    "\n",
    "### 2. **Global Model Initialization:**\n",
    "   - **`input_shape_glob = X_df_scl.shape[1]`**: Gets the input shape for the global model from the dataset (`X_df_scl`).\n",
    "   - **`global_model = create_model(input_shape=input_shape_glob)`**: Creates a global model using the input shape of the dataset.\n",
    "   - **`global_weights = global_model.get_weights()`**: Extracts the initial weights of the global model. These will be updated during the training rounds based on local model updates from the nodes.\n",
    "   - **Performance metrics initialization:** Several lists (`global_accuracies`, `global_precisions`, `global_recalls`, `global_f1s`) are created to store metrics (accuracy, precision, recall, F1-score) for each round of training.\n",
    "\n",
    "### 3. **Initial Evaluation:**\n",
    "   - **`X_test, Y_test = load_data(num_nodes + 1)`**: Loads a test set of data (presumably data not seen by any of the nodes) for evaluating the global model at each round.\n",
    "\n",
    "### 4. **Training Rounds Loop:**\n",
    "   - **`for round in range(num_rounds + 1):`**: This loop runs through `num_rounds` iterations, plus an initial round (round 0).\n",
    "\n",
    "### 5. **Local Training Loop (for each node):**\n",
    "   - **`for node in range(num_nodes):`**: Inside each round, this loop iterates over all nodes (clients). Each node performs local training using its own data.\n",
    "\n",
    "   - **Local Data Loading and Splitting:**\n",
    "     - **`X, Y = load_data(node)`**: Loads the local data for the given node.\n",
    "     - **`X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2)`**: Splits the local data into training and validation sets (80% for training and 20% for validation).\n",
    "   \n",
    "   - **Model Creation and Training:**\n",
    "     - **`model = create_model(input_shape=input_shape)`**: A new local model is created with the input shape of the local training data.\n",
    "     - **`model.set_weights(global_weights)`**: The local model is initialized with the current global model weights.\n",
    "     - **`model.fit(X_train, Y_train, epochs=1, verbose=0)`**: The local model is trained on the node's training data for one epoch.\n",
    "\n",
    "   - **Local Model Evaluation:**\n",
    "     - **`loss, accuracy, precision, recall = model.evaluate(X_val, Y_val, verbose=0)`**: After training, the local model is evaluated on the validation set. Accuracy, precision, and recall are computed.\n",
    "     - **`f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)`**: The F1-score is calculated using the precision and recall (a harmonic mean of precision and recall).\n",
    "\n",
    "   - **Local Weights Collection:**\n",
    "     - **`local_weights.append(model.get_weights())`**: The trained weights of the local model are collected and stored.\n",
    "\n",
    "### 6. **Global Model Weight Aggregation:**\n",
    "   - **`new_weights = [np.mean([weight[layer] for weight in local_weights], axis=0) for layer in range(len(global_weights))]`**:\n",
    "     - The weights of the local models from all nodes are averaged for each layer. This represents the core step of federated learning, where local updates are combined to form a new global model.\n",
    "   - **`global_weights = new_weights`**: The global weights are updated with the aggregated (averaged) weights.\n",
    "   - **`global_model.set_weights(global_weights)`**: The global model is updated with the new aggregated weights.\n",
    "\n",
    "### 7. **Global Model Evaluation:**\n",
    "   - After each round, the global model is evaluated on the test set:\n",
    "     - **`loss, accuracy, precision, recall = model.evaluate(X_test, Y_test, verbose=0)`**: The global model is tested on the global test data.\n",
    "     - **`f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)`**: The F1-score for the global model is calculated.\n",
    "     - **The metrics (`accuracy`, `precision`, `recall`, and `f1_score`) are stored** in their respective lists (`global_accuracies`, `global_precisions`, `global_recalls`, `global_f1s`).\n",
    "\n",
    "### 8. **Return Statement:**\n",
    "   - **`return global_model, global_accuracies, global_precisions, global_recalls, global_f1s`**: After the final round, the function returns the final global model and the lists containing the performance metrics for each round.\n",
    "\n",
    "### Summary:\n",
    "This function implements a federated learning process for CIDS, where:\n",
    "- Each node trains a local model on its data, starting from the global model's weights.\n",
    "- The weights from all nodes are aggregated after each round, updating the global model.\n",
    "- The global model's performance is evaluated and tracked over time using accuracy, precision, recall, and F1-score, ensuring it improves after each round of federated learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74789bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIDS with federated learning training process\n",
    "\n",
    "# change num_nodes and num_rounds for your simulation scenario\n",
    "\n",
    "def cids_federated_training(num_nodes=5, num_rounds=5): \n",
    "    input_shape_glob = X_df_scl.shape[1]\n",
    "    global_model = create_model(input_shape=input_shape_glob)\n",
    "    global_weights = global_model.get_weights()\n",
    "    global_accuracies = []\n",
    "    global_precisions = []\n",
    "    global_recalls = []\n",
    "    global_f1s = []\n",
    "\n",
    "    # Initial evaluation\n",
    "    X_test, Y_test = load_data(num_nodes+1)\n",
    "\n",
    "    # Training rounds\n",
    "    for round in range(num_rounds + 1):\n",
    "        local_weights = []\n",
    "\n",
    "        for node in range(num_nodes):\n",
    "            X, Y = load_data(node)\n",
    "                    \n",
    "            X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2)\n",
    "            \n",
    "            input_shape = X_train.shape[1]\n",
    "\n",
    "            model = create_model(input_shape=input_shape)\n",
    "            model.set_weights(global_weights)\n",
    "            model.fit(X_train, Y_train, epochs=1, verbose=0)\n",
    "\n",
    "            # Validation\n",
    "            loss, accuracy, precision, recall = model.evaluate(X_val, Y_val, verbose=0)\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "            print(f\"Node {node + 1}: Accuracy {accuracy:.4f} - Precision {precision:.4f} - Recall {recall:.4f} - F1-Score {f1_score:.4f}\")\n",
    "\n",
    "            local_weights.append(model.get_weights())\n",
    "\n",
    "        # Aggregate weights\n",
    "        new_weights = [np.mean([weight[layer] for weight in local_weights], axis=0) for layer in range(len(global_weights))]\n",
    "        global_weights = new_weights\n",
    "\n",
    "        global_model.set_weights(global_weights)\n",
    "\n",
    "        # Evaluate global model accuracy\n",
    "        loss, accuracy, precision, recall = model.evaluate(X_test, Y_test, verbose=0)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "        global_accuracies.append(accuracy)\n",
    "        global_precisions.append(precision)\n",
    "        global_recalls.append(recall)\n",
    "        global_f1s.append(f1_score)\n",
    "        \n",
    "        print(f\"\\nRound {round}: Accuracy {accuracy:.4f} - Precision {precision:.4f} - Recall {recall:.4f} - F1-Score {f1_score:.4f}\\n\")\n",
    "\n",
    "    return global_model,, global_accuracies, global_precisions, global_recalls, global_f1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e8005a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Run the simulation, then get the global model and perfromance metric in each round.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CIDS simulator with Non-IID data from single dataset\n",
    "print(\"Simulation for CIDS with Non-IID Data\\n\")\n",
    "fl_model, fl_global_accuracies, fl_global_precisions, fl_global_recalls, fl_global_f1s = cids_federated_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd947b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Plot the performance metric in each round using graph\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting results\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.subplot(4, 2, 1)\n",
    "plt.plot(fl_global_accuracies, label='Accuracy')\n",
    "plt.plot(fl_global_precisions, label='Precision')\n",
    "plt.plot(fl_global_recalls, label='Recall')\n",
    "plt.plot(fl_global_f1s, label='F1-Score')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Value')\n",
    "plt.title('CIDS Non-IID')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
