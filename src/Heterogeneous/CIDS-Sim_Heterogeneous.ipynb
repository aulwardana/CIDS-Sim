{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b8eba81-2688-4a9c-8182-4540037e4f6c",
   "metadata": {},
   "source": [
    "# CIDS-Sim with Non-IID data\n",
    "\n",
    "Federated Learning enables collaboration among multiple clients to learn from decentralized data. In the context of Collaborative Intrusion Detection Systems (CIDS), each client functions as a detector unit distributed across various networks, while the central server, responsible for aggregating the models, acts as the correlation unit. This simulator operates in a non-IID (Non-Independent and Identically Distributed) data setting, where data is distributed differently across clients. The research utilizes multiple datasets, including CoAt_CIC-BoT-IoT-V2.parquet, CoAt_CIC-IDS2017-V2.parquet, CoAt_CIC-ToN-IoT-V2.parquet, CoAt_CIC-UNSW-NB15_Feeded-V2.parquet, and CoAt_CSE-CIC-IDS2018_Feeded.parquet, ensuring that each client works with a distinct dataset. The Federated Averaging (FedAvg) algorithm is employed to aggregate the models from multiple clients.\n",
    "\n",
    "## CIDS Architecture\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"699\" alt=\"image\" src=\"https://github.com/aulwardana/CIDS-Sim/blob/main/images/arch_CIDS-Sim_Non-IID.png?raw=true\">\n",
    "</p>\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "This simulator will use a Coordinated Attack dataset from [here](https://data.mendeley.com/datasets/28tmfg3rzb/2).\n",
    "\n",
    "## Other Information\n",
    "\n",
    "The simulator will run binary classification, so the traffic will labeled as normal (0) or anomaly (1)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5822a7-9129-4bb0-82a7-f9fcf3ef9609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.metrics import Recall, Precision\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091c9451-8a67-4443-aead-c6aab9d7a1e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Load Coordinated Attack dataset. We are using \".parquet\" file for faster reading data.\n",
    "\n",
    "Use heterogeneous and multi network dataset from CoAt-Set to run this simulation.\n",
    "The dataset used the CIC feature for heterogeneous and multi network dataset in the Non-IID scenario.\n",
    "Here is the list of the dataset:\n",
    "1. `CoAt_CIC-BoT-IoT-V2.parquet`\n",
    "2. `CoAt_CIC-IDS2017-V2.parquet`\n",
    "3. `CoAt_CIC-ToN-IoT-V2.parquet`\n",
    "4. `CoAt_CIC-UNSW-NB15_Feeded-V2.parquet`\n",
    "5. `CoAt_CSE-CIC-IDS2018_Feeded.parquet`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087cc6b-0d6a-4d73-bdf0-47ee3c07073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset file\n",
    "file_path0 = os.path.join('..', '..', 'dataset', 'CoAt_CIC-BoT-IoT-V2.parquet')\n",
    "file_path1 = os.path.join('..', '..', 'dataset', 'CoAt_CIC-IDS2017-V2.parquet')\n",
    "file_path2 = os.path.join('..', '..', 'dataset', 'CoAt_CIC-ToN-IoT-V2.parquet')\n",
    "file_path3 = os.path.join('..', '..', 'dataset', 'CoAt_CIC-UNSW-NB15_Feeded-V2.parquet')\n",
    "file_path4 = os.path.join('..', '..', 'dataset', 'CoAt_CSE-CIC-IDS2018_Feeded.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b77c01-95d5-40ad-8c9c-1826b96244b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to read dataset using parquet file (default)\n",
    "df0 = pd.read_parquet('./dataset/CoAt_CIC-BoT-IoT-V2.parquet', engine='pyarrow')\n",
    "df1 = pd.read_parquet('./dataset/CoAt_CIC-IDS2017-V2.parquet', engine='pyarrow')\n",
    "df2 = pd.read_parquet('./dataset/CoAt_CIC-ToN-IoT-V2.parquet', engine='pyarrow')\n",
    "df3 = pd.read_parquet('./dataset/CoAt_CIC-UNSW-NB15_Feeded-V2.parquet', engine='pyarrow')\n",
    "df4 = pd.read_parquet('./dataset/CoAt_CSE-CIC-IDS2018_Feeded.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff395d2e-25b8-4b26-9a8e-851fb34704ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "View the dataset information. Please change the number so you can check every info from each dataset (e.g. df0, df1, df2, df3, and df4)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a644cd-fe9a-4ba6-95c3-74e6da943159",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49ba9c-e1db-489c-b27f-6d438585bef2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Choose a binary label, so we drop the multi-class label\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c1a90-f8a1-493c-a996-040047d44306",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df0.drop(columns=['Label'])\n",
    "df1 = df1.drop(columns=['Label'])\n",
    "df2 = df2.drop(columns=['Label'])\n",
    "df3 = df3.drop(columns=['Label'])\n",
    "df4 = df4.drop(columns=['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa89ae5-7def-441b-8029-76cace307599",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "View the dataset information. Please change the number so you can check every info from each dataset (e.g. df0, df1, df2, df3, and df4)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89c541-1b41-470c-9fe7-89f907fdddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13864881-fe69-4142-a6e3-e369f6321d95",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "View normal (0) and anomaly (1) traffic distribution. Please change the number so you can check every info from each dataset (e.g. df0, df1, df2, df3, and df4)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478a4f8-04c8-45d2-82ae-6c1d24bc047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0['Attack'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2fedf9-a6bd-4fe3-a891-ac667802ed57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "X and y are used to represent the input features and the corresponding target labels, respectively.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee09a923-1172-4e19-9651-b0ec3fc74fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df0 = df0.drop(columns=['Attack'])\n",
    "y_df0 = df0['Attack']\n",
    "\n",
    "X_df1 = df1.drop(columns=['Attack'])\n",
    "y_df1 = df1['Attack']\n",
    "\n",
    "X_df2 = df2.drop(columns=['Attack'])\n",
    "y_df2 = df2['Attack']\n",
    "\n",
    "X_df3 = df3.drop(columns=['Attack'])\n",
    "y_df3 = df3['Attack']\n",
    "\n",
    "X_df4 = df4.drop(columns=['Attack'])\n",
    "y_df4 = df4['Attack']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786374d2-d500-43a7-8e84-fccf0463ba0f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Scaling data to ensures that features have values in the same range\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fccd02-c957-482d-b5c8-4b6d3079dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this scaler for CIC dataset\n",
    "scaler0 = StandardScaler()\n",
    "scaler1 = StandardScaler()\n",
    "scaler2 = StandardScaler()\n",
    "scaler3 = StandardScaler()\n",
    "scaler4 = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fdc04c-a3c2-4013-93d6-5ed1413f9755",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df0_scl = scaler0.fit_transform(X_df0)\n",
    "X_df1_scl = scaler1.fit_transform(X_df1)\n",
    "X_df2_scl = scaler2.fit_transform(X_df2)\n",
    "X_df3_scl = scaler3.fit_transform(X_df3)\n",
    "X_df4_scl = scaler4.fit_transform(X_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0714f6-5ccc-4e19-b2b0-9d2ee6dfcc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for each client dataset. \n",
    "# If you have other dataset that you want to add or you want to decrease the number of dataset, you can modify 'list_data' and 'list_label'\n",
    "list_data = {\n",
    "    0: X_df0_scl,\n",
    "    1: X_df1_scl,\n",
    "    2: X_df2_scl,\n",
    "    3: X_df3_scl,\n",
    "    4: X_df4_scl\n",
    "}\n",
    "\n",
    "list_label = {\n",
    "    0: y_df0,\n",
    "    1: y_df1,\n",
    "    2: y_df2,\n",
    "    3: y_df3,\n",
    "    4: y_df4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69afd0f5-39d8-49f7-b7bd-5febfca5ac36",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This Python function, `load_data(client_id)`, is designed to load a portion of data for a specific client in a Federated Learning setting where non-IID data is distributed among different clients. That process is designed to divide a large dataset into smaller portions for different clients.\n",
    "\n",
    "Please change `fraction` variable if you want to change the data portion that will distribute to each client.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736d82b-3023-4ae1-a2b2-92ce3894b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(client_id):\n",
    "    X_df_scl = list_data[client_id]\n",
    "    y_df = y_df = pd.Series(list_label[client_id].values, index=list_label[client_id].index, name=\"Attack\")\n",
    "    \n",
    "    # Create non-IID splits based on client_id\n",
    "    np.random.seed(client_id)\n",
    "    indices = np.arange(len(X_df_scl))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Choose a fraction of the data for this client\n",
    "    fraction = 0.2\n",
    "    client_data_size = int(fraction * len(X_df_scl))\n",
    "    client_indices = indices[:client_data_size]\n",
    "\n",
    "    X_client = X_df_scl[client_indices]\n",
    "    y_client = y_df.iloc[client_indices]\n",
    "\n",
    "    return X_client, y_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c14ed0-8d61-49ef-86aa-03d6e9ef0b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glob_test_data():\n",
    "    X_client, y_client = [], []\n",
    "\n",
    "    for i in range(5):\n",
    "        X_df_scl = list_data[i]\n",
    "        y_df = pd.Series(list_label[i].values, index=list_label[i].index, name=\"Attack\")\n",
    "\n",
    "        # Define fraction for testing data\n",
    "        test_size = int(0.1 * len(X_df_scl))  # Use 10% for testing\n",
    "        indices = np.arange(len(X_df_scl))\n",
    "        np.random.shuffle(indices)\n",
    "        test_indices = indices[:test_size]\n",
    "\n",
    "        X_client.append(X_df_scl[test_indices])\n",
    "        y_client.append(y_df.iloc[test_indices])\n",
    "\n",
    "    return X_client, y_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac70db0-3b8e-4bcd-9bc5-52611cf9ab17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The `create_model()` function defines a simple deep neural network model using Keras.\n",
    "You can change the **number of neurons** in each **Dense layer**.\n",
    "You can also experiment by changing the **Activation function, Loss function, and optimizer** from the deep learning model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f5c24b-3db9-4b08-b0aa-e87301028014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple deep neural network model\n",
    "def create_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(37, activation='relu', input_shape=(input_shape,)),\n",
    "        layers.Dense(18, activation='relu'),\n",
    "        layers.Dense(9, activation='relu'),\n",
    "        layers.Dense(4, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy', Recall(), Precision()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bf886c-4786-4fe5-b350-a61da6a1fe31",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Calculate model size to measure the overhead in communication\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3db05-58c9-4a0c-a2c9-d3191f34bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_size(model):\n",
    "    \"\"\"Calculate the size of a model (in bytes) by summing the number of parameters.\"\"\"\n",
    "    total_params = np.sum([np.prod(weights.shape) for weights in model.get_weights()])\n",
    "    size_in_bytes = total_params * 4  # Assuming 32-bit float (4 bytes per parameter)\n",
    "    return size_in_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd7df5-0402-41d6-b96f-99a9454c4ca2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The `cids_federated_training()` function implements the **training process for a Collaborative Intrusion Detection System (CIDS) using Federated Learning**. The goal is to train a global model based on the local training of models across multiple distributed nodes, without sharing raw data. \n",
    "\n",
    "This function performs federated learning for intrusion detection across `num_nodes` (clients or devices) over `num_rounds` (iterations of federated learning). You can experiment by changing `num_nodes` and `num_rounds` from this function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c0db9-ccce-4714-93ee-18ab1be3f10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIDS with federated learning training process\n",
    "\n",
    "# change num_nodes and num_rounds for your simulation scenario\n",
    "\n",
    "def cids_federated_training(num_nodes=5, num_rounds=5): \n",
    "    feature_counts = [X_df0_scl.shape[1], X_df1_scl.shape[1], X_df2_scl.shape[1], X_df3_scl.shape[1], X_df4_scl.shape[1]]\n",
    "    input_shape_glob = int(np.mean(feature_counts))\n",
    "    global_model = create_model(input_shape=input_shape_glob)\n",
    "    global_weights = global_model.get_weights()\n",
    "\n",
    "    global_pred_times = []\n",
    "\n",
    "    # Calculate model size (bytes per round of communication)\n",
    "    model_size = calculate_model_size(global_model)\n",
    "    print(f\"Model size: {model_size / 1e6:.2f} MB\")\n",
    "\n",
    "    communication_overhead = 0\n",
    "\n",
    "    #Global model performance evaluation\n",
    "    global_accuracies = []\n",
    "    global_precisions = []\n",
    "    global_recalls = []\n",
    "    global_f1s = []\n",
    "\n",
    "    # Variable for global training and prediction time\n",
    "    total_training_times = []\n",
    "    total_prediction_times = []\n",
    "\n",
    "    # Variable for global training CPU and Memory usage\n",
    "    cpu_usages = []\n",
    "    memory_usages = []\n",
    "\n",
    "    # Variable for global variance in performance across nodes\n",
    "    accuracy_variances = []\n",
    "    precision_variances = []\n",
    "    recall_variances = []\n",
    "    f1_variances = []\n",
    "\n",
    "    # Variable for global standard deviation of performance\n",
    "    accuracy_stds = []\n",
    "    precision_stds = []\n",
    "    recall_stds = []\n",
    "    f1_stds = []\n",
    "\n",
    "    # Initial evaluation\n",
    "    X_test, Y_test = load_glob_test_data()\n",
    "\n",
    "    # Initialize a dictionary to store test accuracies, precision, recall, and F1-Score for each client across rounds\n",
    "    test_accuracies_per_client = {i: [] for i in range(num_nodes)}  \n",
    "    test_precisions_per_client = {i: [] for i in range(num_nodes)} \n",
    "    test_recalls_per_client = {i: [] for i in range(num_nodes)}\n",
    "    test_f1s_per_client = {i: [] for i in range(num_nodes)}\n",
    "\n",
    "    # Training rounds\n",
    "    for round in range(num_rounds):\n",
    "        local_weights = []\n",
    "        local_training_times = []\n",
    "        local_prediction_times = []\n",
    "\n",
    "        #Local model performance evaluation\n",
    "        local_accuracies = []\n",
    "        local_precisions = []\n",
    "        local_recalls = []\n",
    "        local_f1s = []\n",
    "\n",
    "        # Local CPU and Memory usage\n",
    "        round_cpu_usage = []\n",
    "        round_memory_usage = []\n",
    "\n",
    "        print(f\"\\n------------------------------------------------------------\\n\")\n",
    "        print(f\"Training Round {round + 1}\\n\")\n",
    "\n",
    "        for node in range(num_nodes):\n",
    "            X, Y = load_data(node)\n",
    "                    \n",
    "            X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2)\n",
    "            \n",
    "            input_shape = X_train.shape[1]\n",
    "\n",
    "            model = create_model(input_shape=input_shape)\n",
    "            model.set_weights(global_weights)\n",
    "\n",
    "            # Measure CPU and memory utilization during training\n",
    "            cpu_before = psutil.cpu_percent(interval=None)\n",
    "            memory_before = psutil.virtual_memory().percent\n",
    "\n",
    "            # Measure local training time\n",
    "            start_train_time = time.time()\n",
    "            #local training time\n",
    "            model.fit(X_train, Y_train, epochs=10, batch_size=1000, verbose=0)\n",
    "            end_train_time = time.time()\n",
    "\n",
    "            cpu_after = psutil.cpu_percent(interval=None)\n",
    "            memory_after = psutil.virtual_memory().percent\n",
    "\n",
    "            local_training_time = end_train_time - start_train_time\n",
    "            local_training_times.append(local_training_time)\n",
    "\n",
    "            cpu_usage = cpu_after - cpu_before\n",
    "            memory_usage = memory_after - memory_before\n",
    "\n",
    "            print(f\"Node {node + 1}: Training Time {local_training_time:.4f} seconds\")\n",
    "            print(f\"Node {node + 1}: CPU Usage {cpu_usage:.2f}% - Memory Usage {memory_usage:.2f}%\")\n",
    "\n",
    "            # Measure local resource consumption\n",
    "            round_cpu_usage.append(cpu_usage)\n",
    "            round_memory_usage.append(memory_usage)\n",
    "\n",
    "            # Measure local prediction time\n",
    "            start_pred_time = time.time()\n",
    "            # Validation\n",
    "            loss, accuracy, precision, recall = model.evaluate(X_val, Y_val, verbose=0)\n",
    "            end_pred_time = time.time()\n",
    "\n",
    "            local_prediction_time = end_pred_time - start_pred_time\n",
    "            local_prediction_times.append(local_prediction_time)\n",
    "\n",
    "            print(f\"Node {node + 1}: Prediction Time {local_prediction_time:.4f} seconds\")\n",
    "            \n",
    "            f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "            print(f\"Node {node + 1}: Accuracy {accuracy:.4f} - Precision {precision:.4f} - Recall {recall:.4f} - F1-Score {f1_score:.4f}\\n\")\n",
    "\n",
    "            #Append local performance data\n",
    "            local_accuracies.append(accuracy)\n",
    "            local_precisions.append(precision)\n",
    "            local_recalls.append(recall)\n",
    "            local_f1s.append(f1_score)\n",
    "            \n",
    "            local_weights.append(model.get_weights())\n",
    "\n",
    "            # Communication: Server receiving weights from each node\n",
    "            communication_overhead += model_size\n",
    "\n",
    "        # Measure server aggregation time\n",
    "        start_aggregation_time = time.time()\n",
    "        # Aggregate weights\n",
    "        new_weights = [np.mean([weight[layer] for weight in local_weights], axis=0) for layer in range(len(global_weights))]\n",
    "        end_aggregation_time = time.time()\n",
    "        \n",
    "        aggregation_time = end_aggregation_time - start_aggregation_time\n",
    "        print(f\"Aggregation Time round {round + 1}: {aggregation_time:.4f} seconds\")\n",
    "        \n",
    "        global_weights = new_weights\n",
    "        global_model.set_weights(global_weights)\n",
    "\n",
    "        # Communication: Server sending updated weights to all nodes\n",
    "        communication_overhead += num_nodes * model_size\n",
    "\n",
    "        print(f\"Total communication overhead after round {round + 1}: {communication_overhead / 1e6:.2f} MB\")\n",
    "\n",
    "        tmp_acc = []\n",
    "        tmp_pre = []\n",
    "        tmp_rec = []\n",
    "        tmp_f1 = []\n",
    "        tmp_time = []\n",
    "        for client_id in range(num_nodes):\n",
    "            # Evaluate and measure the time\n",
    "            start_glob_pred_time = time.time()\n",
    "            loss, accuracy, precision, recall = global_model.evaluate(X_test[client_id], Y_test[client_id], verbose=0)\n",
    "            end_glob_pred_time = time.time()\n",
    "\n",
    "            tmp_time.append(end_glob_pred_time - start_glob_pred_time)\n",
    "            \n",
    "            f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "            test_accuracies_per_client[client_id].append(accuracy)\n",
    "            tmp_acc.append(accuracy) \n",
    "            test_precisions_per_client[client_id].append(precision)\n",
    "            tmp_pre.append(precision)\n",
    "            test_recalls_per_client[client_id].append(recall)\n",
    "            tmp_rec.append(recall)\n",
    "            test_f1s_per_client[client_id].append(f1_score)\n",
    "            tmp_f1.append(f1_score)\n",
    "\n",
    "        global_pred_time = np.mean(tmp_time)\n",
    "        global_pred_times.append(global_pred_time)\n",
    "        print(f\"\\nAverage Global Prediction Time Round {round + 1}: {global_pred_time:.4f}\")\n",
    "        \n",
    "        # Evaluate global model accuracy\n",
    "        global_accuracies.append(np.mean(tmp_acc))\n",
    "        global_precisions.append(np.mean(tmp_pre))\n",
    "        global_recalls.append(np.mean(tmp_rec))\n",
    "        global_f1s.append(np.mean(tmp_f1))\n",
    "        \n",
    "        print(f\"\\nRound {round + 1} Average: Accuracy {np.mean(tmp_acc):.4f} - Precision {np.mean(tmp_pre):.4f} - Recall {np.mean(tmp_rec):.4f} - F1-Score {np.mean(tmp_f1):.4f}\\n\")\n",
    "\n",
    "        # Cross-node Generalization Calculation\n",
    "        accuracy_variance = np.var(local_accuracies)\n",
    "        accuracy_variances.append(accuracy_variance)\n",
    "        precision_variance = np.var(local_precisions)\n",
    "        precision_variances.append(precision_variance)\n",
    "        recall_variance = np.var(local_recalls)\n",
    "        recall_variances.append(recall_variance)\n",
    "        f1_variance = np.var(local_f1s)\n",
    "        f1_variances.append(f1_variance)\n",
    "\n",
    "        accuracy_std = np.std(local_accuracies)\n",
    "        accuracy_stds.append(accuracy_std)\n",
    "        precision_std = np.std(local_precisions)\n",
    "        precision_stds.append(precision_std)\n",
    "        recall_std = np.std(local_recalls)\n",
    "        recall_stds.append(recall_std)\n",
    "        f1_std = np.std(local_f1s)\n",
    "        f1_stds.append(f1_std)\n",
    "\n",
    "        print(f\"Cross-node Generalization after Round {round + 1}:\")\n",
    "        print(f\"Accuracy Variance: {accuracy_variance:.4f}, Accuracy Std: {accuracy_std:.4f}\")\n",
    "        print(f\"Precision Variance: {precision_variance:.4f}, Precision Std: {precision_std:.4f}\")\n",
    "        print(f\"Recall Variance: {recall_variance:.4f}, Recall Std: {recall_std:.4f}\")\n",
    "        print(f\"F1-Score Variance: {f1_variance:.4f}, F1-Score Std: {f1_std:.4f}\\n\")\n",
    "\n",
    "        # Calculate total training time for the round\n",
    "        total_training_time = sum(local_training_times) + aggregation_time\n",
    "        total_training_times.append(total_training_time)\n",
    "\n",
    "        print(f\"\\nTotal Training Time Round {round + 1}: {total_training_time:.4f}\")\n",
    "\n",
    "        # Calculate total prediction time for the round\n",
    "        total_prediction_time = sum(local_prediction_times)\n",
    "        total_prediction_times.append(total_prediction_time)\n",
    "\n",
    "        print(f\"\\nTotal Prediction Time Round {round + 1}: {total_prediction_time:.4f}\")\n",
    "\n",
    "        # Append round-level resource utilization\n",
    "        cpu_usages.append(np.mean(round_cpu_usage))\n",
    "        memory_usages.append(np.mean(round_memory_usage))\n",
    "\n",
    "    return global_model, global_accuracies, global_precisions, global_recalls, global_f1s, test_accuracies_per_client, test_precisions_per_client, test_recalls_per_client, test_f1s_per_client, communication_overhead, total_training_times, total_prediction_times, global_pred_times, cpu_usages, memory_usages, accuracy_variances, precision_variances, recall_variances, f1_variances, accuracy_stds, precision_stds, recall_stds, f1_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1356673f-0053-4968-a9c5-ed0ecbbc99ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Run the simulation, then get the global model and perfromance metric in each round.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ffb793-5d1c-4c46-8c32-685fdd520526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CIDS simulator with Non-IID data from single dataset\n",
    "print(\"Simulation for CIDS with Non-IID Data\\n\")\n",
    "fl_model, fl_global_accuracies, fl_global_precisions, fl_global_recalls, fl_global_f1s, test_accuracies_per_client, test_precisions_per_client, test_recalls_per_client, test_f1s_per_client, communication_overhead, total_training_times, total_prediction_times, global_pred_times, cpu_usages, memory_usages, accuracy_variances, precision_variances, recall_variances, f1_variances, accuracy_stds, precision_stds, recall_stds, f1_stds = cids_federated_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7afb47f-23bb-4a42-a526-3a579834f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nFinal Communication Overhead: {communication_overhead / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b53fc-14fb-4441-af4f-c5c2763511aa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Plot the performance metric in each round using graph\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b4167c-2f43-4afd-916b-ee7f21119dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting model performance results\n",
    "plt.figure(figsize=(28, 20))\n",
    "\n",
    "plt.subplot(4, 2, 1)\n",
    "rounds1 = np.arange(1, len(fl_global_accuracies) + 1)\n",
    "plt.plot(rounds1, fl_global_accuracies, label='Accuracy')\n",
    "plt.plot(rounds1, fl_global_precisions, label='Precision')\n",
    "plt.plot(rounds1, fl_global_recalls, label='Recall')\n",
    "plt.plot(rounds1, fl_global_f1s, label='F1-Score')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Value')\n",
    "plt.title('CIDS Heterogeneous Non-IID FL Model Performance')\n",
    "plt.gca().xaxis.set_major_locator(mticker.MaxNLocator(integer=True))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8f166f-5d29-477f-89b8-e295815c45ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of test accuracy over rounds\n",
    "num_rounds = 5\n",
    "plt.figure(figsize=(20, 10))\n",
    "for client_id, accuracies in test_accuracies_per_client.items():\n",
    "    plt.plot(range(1, num_rounds + 1), accuracies, label=f'Client {client_id}')\n",
    "\n",
    "plt.xlabel('Training Round')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Global Model Test Accuracy on Each Client\\'s Test Data Over Rounds')\n",
    "plt.gca().xaxis.set_major_locator(mticker.MaxNLocator(integer=True))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad72a3-0f48-4496-adaf-a08feecdf9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of test precision over rounds\n",
    "num_rounds = 5\n",
    "plt.figure(figsize=(20, 10))\n",
    "for client_id, precisions in test_precisions_per_client.items():\n",
    "    plt.plot(range(1, num_rounds + 1), precisions, label=f'Client {client_id}')\n",
    "\n",
    "plt.xlabel('Training Round')\n",
    "plt.ylabel('Test Precision')\n",
    "plt.title('Global Model Test Precision on Each Client\\'s Test Data Over Rounds')\n",
    "plt.gca().xaxis.set_major_locator(mticker.MaxNLocator(integer=True))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6750bf-fc91-41f8-9345-2200a68ffe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of test recall over rounds\n",
    "num_rounds = 5\n",
    "plt.figure(figsize=(20, 10))\n",
    "for client_id, recalls in test_recalls_per_client.items():\n",
    "    plt.plot(range(1, num_rounds + 1), recalls, label=f'Client {client_id}')\n",
    "\n",
    "plt.xlabel('Training Round')\n",
    "plt.ylabel('Test Recall')\n",
    "plt.title('Global Model Test Recall on Each Client\\'s Test Data Over Rounds')\n",
    "plt.gca().xaxis.set_major_locator(mticker.MaxNLocator(integer=True))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b9142-bab8-4e85-a37a-166333152bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of test f1 over rounds\n",
    "num_rounds = 5\n",
    "plt.figure(figsize=(20, 10))\n",
    "for client_id, f1s in test_f1s_per_client.items():\n",
    "    plt.plot(range(1, num_rounds + 1), f1s, label=f'Client {client_id}')\n",
    "\n",
    "plt.xlabel('Training Round')\n",
    "plt.ylabel('Test F1-Score')\n",
    "plt.title('Global Model Test F1-Score on Each Client\\'s Test Data Over Rounds')\n",
    "plt.gca().xaxis.set_major_locator(mticker.MaxNLocator(integer=True))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732fa6e5-8255-4189-9ae8-ce40e3d91918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting model robustness and generalization results\n",
    "plt.figure(figsize=(28, 20))\n",
    "\n",
    "plt.subplot(4, 2, 1)\n",
    "rounds2 = np.arange(1, len(accuracy_variances) + 1)\n",
    "plt.plot(rounds2, accuracy_variances, label='Accuracy')\n",
    "plt.plot(rounds2, precision_variances, label='Precision')\n",
    "plt.plot(rounds2, recall_variances, label='Recall')\n",
    "plt.plot(rounds2, f1_variances, label='F1-Score')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Value')\n",
    "plt.title('CIDS Heterogeneous Non-IID FL Variance in Model Performance Across Nodes')\n",
    "plt.gca().xaxis.set_major_locator(mticker.MaxNLocator(integer=True))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b204b-3384-4527-81af-e083717438db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting model robustness and generalization results\n",
    "plt.figure(figsize=(28, 20))\n",
    "\n",
    "plt.subplot(4, 2, 1)\n",
    "rounds3 = np.arange(1, len(accuracy_stds) + 1)\n",
    "plt.plot(rounds3, accuracy_stds, label='Accuracy')\n",
    "plt.plot(rounds3, precision_stds, label='Precision')\n",
    "plt.plot(rounds3, recall_stds, label='Recall')\n",
    "plt.plot(rounds3, f1_stds, label='F1-Score')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Value')\n",
    "plt.title('CIDS Heterogeneous Non-IID FL Standard Deviation of Model Performance')\n",
    "plt.gca().xaxis.set_major_locator(mticker.MaxNLocator(integer=True))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb63bb-cad2-4c44-987c-fc145e6e76eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training and prediction time\n",
    "plt.figure(figsize=(28, 20))\n",
    "\n",
    "plt.subplot(4, 2, 1)\n",
    "rounds4 = np.arange(1, len(total_training_times) + 1)\n",
    "plt.plot(rounds4, total_training_times, label='Training Time')\n",
    "plt.plot(rounds4, total_prediction_times, label='Prediction Time')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Value')\n",
    "plt.title('CIDS Heterogeneous Non-IID FL Training and Prediction Time in Each Round')\n",
    "plt.gca().xaxis.set_major_locator(mticker.MaxNLocator(integer=True))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2929db-ccf2-4f52-989c-ae801bf18412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting resource consumption\n",
    "plt.figure(figsize=(28, 20))\n",
    "\n",
    "plt.subplot(4, 2, 1)\n",
    "rounds5 = np.arange(1, len(cpu_usages) + 1)\n",
    "plt.plot(rounds5, cpu_usages, label='CPU')\n",
    "plt.plot(rounds5, memory_usages, label='RAM')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Value')\n",
    "plt.title('CIDS Heterogeneous Non-IID FL Resource Consumption')\n",
    "plt.gca().xaxis.set_major_locator(mticker.MaxNLocator(integer=True))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f39d16-c12a-4fff-ad3a-60b475ba94d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting global prediction time\n",
    "plt.figure(figsize=(28, 20))\n",
    "\n",
    "plt.subplot(4, 2, 1)\n",
    "rounds6 = np.arange(1, len(global_pred_times) + 1)\n",
    "plt.plot(rounds6, global_pred_times, label='Avg Glob Pred Time')\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Round')\n",
    "plt.title('CIDS Heterogeneous Non-IID FL Average Global Prediction Time')\n",
    "plt.gca().xaxis.set_major_locator(mticker.MaxNLocator(integer=True))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e930fe-c7d4-4abc-945b-2710f30058e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Save the performance metric in each round in CSV file\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a12d06-596c-48b4-a203-6f852f8c8975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rounds\n",
    "rounds = list(range(1, len(fl_global_accuracies) + 1))\n",
    "\n",
    "# Save global performance metrics with rounds\n",
    "global_metrics_df = pd.DataFrame({\n",
    "    \"Round\": rounds,\n",
    "    \"Global Accuracy\": fl_global_accuracies,\n",
    "    \"Global Precision\": fl_global_precisions,\n",
    "    \"Global Recall\": fl_global_recalls,\n",
    "    \"Global F1-Score\": fl_global_f1s,\n",
    "    \"Global Prediction Time\": global_pred_times,\n",
    "    \"Total Training Times\": total_training_times,\n",
    "    \"Total Prediction Times\": total_prediction_times,\n",
    "    \"CPU Usage\": cpu_usages,\n",
    "    \"Memory Usage\": memory_usages,\n",
    "    \"Accuracy Variance\": accuracy_variances,\n",
    "    \"Precision Variance\": precision_variances,\n",
    "    \"Recall Variance\": recall_variances,\n",
    "    \"F1 Variance\": f1_variances,\n",
    "    \"Accuracy Std\": accuracy_stds,\n",
    "    \"Precision Std\": precision_stds,\n",
    "    \"Recall Std\": recall_stds,\n",
    "    \"F1 Std\": f1_stds\n",
    "})\n",
    "\n",
    "# Export global metrics to a CSV\n",
    "global_metrics_df.to_csv(\"global_performance_metrics_CIDS_Non-IID_Heterogen.csv\", index=False)\n",
    "\n",
    "# Save client-specific metrics across rounds with rounds as rows\n",
    "test_accuracies_df = pd.DataFrame(test_accuracies_per_client)\n",
    "test_precisions_df = pd.DataFrame(test_precisions_per_client)\n",
    "test_recalls_df = pd.DataFrame(test_recalls_per_client)\n",
    "test_f1s_df = pd.DataFrame(test_f1s_per_client)\n",
    "\n",
    "# Add round information as a column\n",
    "test_accuracies_df.insert(0, \"Round\", rounds)\n",
    "test_precisions_df.insert(0, \"Round\", rounds)\n",
    "test_recalls_df.insert(0, \"Round\", rounds)\n",
    "test_f1s_df.insert(0, \"Round\", rounds)\n",
    "\n",
    "# Export each client metric to a separate CSV\n",
    "test_accuracies_df.to_csv(\"test_accuracies_per_client_CIDS_Non-IID_Heterogen.csv\", index=False)\n",
    "test_precisions_df.to_csv(\"test_precisions_per_client_CIDS_Non-IID_Heterogen.csv\", index=False)\n",
    "test_recalls_df.to_csv(\"test_recalls_per_client_CIDS_Non-IID_Heterogen.csv\", index=False)\n",
    "test_f1s_df.to_csv(\"test_f1s_per_client_CIDS_Non-IID_Heterogen.csv\", index=False)\n",
    "\n",
    "print(\"All metrics exported to CSV files successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
